{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bUs4CsULm-X2"
   },
   "outputs": [],
   "source": [
    "# Copyright (c) 2022, salesforce.com, inc.\n",
    "# All rights reserved.\n",
    "# SPDX-License-Identifier: BSD-3-Clause\n",
    "# For full license text, see the LICENSE file in the repo root or https://opensource.org/licenses/BSD-3-Clause"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guAIuLb3m-X6"
   },
   "source": [
    "This notebook can be run directly on Google Colab. \n",
    "\n",
    "Notice that you need to specify the directory/filename to save the processed documents and tokens.\n",
    "\n",
    "To read or write files on Google Drive, you can run the code below to mount your Google Drive in the notebook:\n",
    "\n",
    "```\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "```\n",
    "\n",
    "\n",
    "Then you can specify the path to the file on the Google drive by a path string starting with \"/content/drive/MyDrive/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7997,
     "status": "ok",
     "timestamp": 1646683078698,
     "user": {
      "displayName": "Jun Yuan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIntxgfKz0b8Rui-mNJd1XjcuuHGqiXsVIbFH_Ew=s64",
      "userId": "16110568450908833581"
     },
     "user_tz": 300
    },
    "id": "q-S23XDE6h40",
    "outputId": "cf4c126b-d94e-461e-950a-8d9ce65dde60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.18.4)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.63.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.2)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
      "Requirement already satisfied: pyarrow!=4.0.0,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2022.2.0)\n",
      "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.10.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.7.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6530,
     "status": "ok",
     "timestamp": 1646683085218,
     "user": {
      "displayName": "Jun Yuan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIntxgfKz0b8Rui-mNJd1XjcuuHGqiXsVIbFH_Ew=s64",
      "userId": "16110568450908833581"
     },
     "user_tz": 300
    },
    "id": "rm24xPpSDmwp",
    "outputId": "5e329805-4d60-47bb-fbc0-8474ff584181"
   },
   "outputs": [],
   "source": [
    "'''load mnli data'''\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('multi_nli', split='validation_matched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FxmptMdbavO"
   },
   "source": [
    "# 1. Testing Data processing\n",
    "Here we transform the original test set (MNLI, travel genre) into a tokens (unigram, bigram, trigram) and a matrix for the occurrence of each token in each instance.\n",
    "\n",
    "The tokens in the test set we extracted in this notebook, will then be used to generate error rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5xWPePbFIol"
   },
   "outputs": [],
   "source": [
    "genre_to_test = ['travel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1646683085220,
     "user": {
      "displayName": "Jun Yuan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIntxgfKz0b8Rui-mNJd1XjcuuHGqiXsVIbFH_Ew=s64",
      "userId": "16110568450908833581"
     },
     "user_tz": 300
    },
    "id": "vVmTV6EnWNWo",
    "outputId": "69236525-f189-4d79-f707-05010885146e"
   },
   "outputs": [],
   "source": [
    "data_by_genre = {\n",
    "    'travel': dataset.filter(lambda x: x['genre']=='travel'),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LM8-9F-gtX6S"
   },
   "source": [
    " #### Save doc.jsonl\n",
    "With the test data saved in a file, the data can be reused by reading doc.jsonl instead of downloading. \n",
    "  \n",
    "The `doc.jsonl` can also be used for document detail view in the user interface by transforming it into a typical json format, \n",
    "\n",
    "that is, {\"content\": docs}, where `docs` is the actual output by running the `df.to_json()` below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CiCCyJFycq9l"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "label_map = {0: 'entailment', 1: 'neutral', 2: 'contradiction'}\n",
    "\n",
    "for genre in ['travel']:\n",
    "  df = pd.DataFrame()\n",
    "  df['sentence1'] = data_by_genre[genre]['premise']\n",
    "  df['sentence2'] = data_by_genre[genre]['hypothesis']\n",
    "\n",
    "  label_list = [label_map[x] for x in data_by_genre[genre]['label']]\n",
    "  df['gold_label'] = label_list\n",
    "  df.to_json(path_or_buf=\"<specify your path>/mnli_government_travel/doc.jsonl\", orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9v8TkUzecRAN"
   },
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WgygTtddUep"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from gensim.models.phrases import Phrases\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "''' you can remove stop words, but we keep them in this example'''\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.matutils import corpus2dense, corpus2csc\n",
    "\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SCec3hNycP1d"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def tokenization(df):\n",
    "  linked = [x['sentence1']+\" <S> \"+x['sentence2'] for i,x in df.iterrows()]\n",
    "  '''tokenization'''\n",
    "  data_word_list = [simple_preprocess(sentence) for sentence in linked]\n",
    "\n",
    "  print(\"length of data_word_list: \" , len(data_word_list))\n",
    "  print(\"length of data_word_list[0]: \" , len(data_word_list[0]))\n",
    "\n",
    "  '''try lemmatization'''\n",
    "  # Initialize spacy 'en' model, keeping only tagger component needed for lemmatization\n",
    "  allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']\n",
    "  data_ready = []\n",
    "\n",
    "  for sent in data_word_list:\n",
    "      # Parse the sentence using the loaded 'en' model object `nlp`. Extract the lemma for each token and join\n",
    "      doc = nlp(\" \".join(sent)) \n",
    "      data_ready.append([token.lemma_ for token in doc])\n",
    "      \n",
    "  # remove stopwords once more after lemmatization\n",
    "  data_ready = [[word for word in simple_preprocess(str(doc))] for doc in data_ready]\n",
    "\n",
    "  bigram = Phrases(data_ready)\n",
    "  bigram_sentences = [bigram[sent] for sent in data_ready]\n",
    "\n",
    "  trigram = Phrases(bigram_sentences)\n",
    "  trigram_sent = [trigram[sent] for sent in bigram_sentences]\n",
    "  return trigram_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ila9_XsgpHQ6"
   },
   "source": [
    "### Create dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zqsLeomdkozf"
   },
   "outputs": [],
   "source": [
    "def corpus_matrix(trigram_sent):\n",
    "  # Create Dictionary\n",
    "  id2word = corpora.Dictionary(trigram_sent)\n",
    "  # Create Corpus: Term Document Frequency\n",
    "  corpus = [id2word.doc2bow(text) for text in trigram_sent]\n",
    "\n",
    "  num_docs = id2word.num_docs\n",
    "  num_terms = len(id2word.keys())\n",
    "  print(\"num_docs, num_terms:\", num_docs, num_terms)\n",
    "\n",
    "  model = TfidfModel(corpus)\n",
    "  corpus_tfidf = model[corpus]\n",
    "  corpus_tfidf_dense = corpus2dense(corpus_tfidf, num_terms, num_docs)\n",
    "  print(\"tfidf shape:\", corpus_tfidf_dense.shape)\n",
    "\n",
    "  ori_columns = []\n",
    "  for i in id2word.keys():\n",
    "      ori_columns.append(id2word[i])\n",
    "\n",
    "  tfidf_sum = corpus_tfidf_dense.sum(axis=1)\n",
    "  df0 = pd.DataFrame(data=corpus_tfidf_dense, index=ori_columns)\n",
    "  median_val = np.median(tfidf_sum)\n",
    "  \n",
    "  input_col = [ori_columns[i] for i in range(len(ori_columns)) if tfidf_sum[i]>=median_val]\n",
    "  corpus_binary_danse = (df0.loc[input_col].values>0).astype(int)\n",
    "  input_columns = input_col \n",
    "\n",
    "  A = np.matrix(corpus_binary_danse.T)\n",
    "  denseA = sparse.csr_matrix(A)\n",
    "  return denseA, input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 856,
     "status": "ok",
     "timestamp": 1646683438821,
     "user": {
      "displayName": "Jun Yuan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIntxgfKz0b8Rui-mNJd1XjcuuHGqiXsVIbFH_Ew=s64",
      "userId": "16110568450908833581"
     },
     "user_tz": 300
    },
    "id": "ZMY2lZAVY6hB",
    "outputId": "0e986076-bb6a-4399-de45-ee3c5a9e5b33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== travel ==========\n",
      "num_docs, num_terms: 1976 4917\n",
      "tfidf shape: (4917, 1976)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for genre in genre_to_test:\n",
    "  '''\n",
    "    Read data from jsonl file.\n",
    "    You can also load the data using datasets library as we did in the 01-model_output+shap.ipynb notebook\n",
    "  '''\n",
    "  print(\"=\"*10, genre, \"=\"*10)\n",
    "  df = pd.read_json(path_or_buf=\"<specify your path>/mnli_government_travel/doc.jsonl\", orient=\"records\")\n",
    "  grams = tokenization(df)\n",
    "\n",
    "  denseA, input_columns = corpus_matrix(grams)\n",
    "  \n",
    "  with open(\"<specify your path>/mnli_government_travel/input_columns.json\", 'w') as output:\n",
    "    output.write(json.dumps({'input_columns': input_columns}))\n",
    "  \n",
    "  sparse.save_npz(\"<specify your path>/mnli_government_travel/corpus_mat.npz\", denseA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zJjUaNHk2yYC"
   },
   "source": [
    "# 2. Training Data Process\n",
    "Here we calculate how each token in the training data set is relevant to each class in terms of the ground truth.\n",
    "\n",
    "The information about training data will then be used for interpreting errors in the user interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 819,
     "status": "ok",
     "timestamp": 1646683454092,
     "user": {
      "displayName": "Jun Yuan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIntxgfKz0b8Rui-mNJd1XjcuuHGqiXsVIbFH_Ew=s64",
      "userId": "16110568450908833581"
     },
     "user_tz": 300
    },
    "id": "mUL8rUn43FP1",
    "outputId": "f76d6d41-891f-4771-9496-54b9eec7d024"
   },
   "outputs": [],
   "source": [
    "dataset_train = load_dataset('multi_nli', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1057979,
     "status": "ok",
     "timestamp": 1646684575576,
     "user": {
      "displayName": "Jun Yuan",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhIntxgfKz0b8Rui-mNJd1XjcuuHGqiXsVIbFH_Ew=s64",
      "userId": "16110568450908833581"
     },
     "user_tz": 300
    },
    "id": "rnixlBSGiQQh",
    "outputId": "44d9991f-ca45-4fa0-e0a8-efa1f3ca5551"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/multi_nli/default/0.0.0/591f72eb6263d1ab527561777936b199b714cda156d35716881158a2bd144f39/cache-12a2fb40d31d6a28.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== travel ==========\n",
      "length of data_word_list:  77350\n",
      "length of data_word_list[0]:  46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
      "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
     ]
    }
   ],
   "source": [
    "for genre in genre_to_test:\n",
    "  print(\"=\"*10, genre, \"=\"*10)\n",
    "  data = dataset_train.filter(lambda x: x['genre']==genre)\n",
    "\n",
    "  df = pd.DataFrame()\n",
    "  df['sentence1'] = data['premise']\n",
    "  df['sentence2'] = data['hypothesis']\n",
    "  trigram_sent = tokenization(df)\n",
    "\n",
    "  id2word = corpora.Dictionary(trigram_sent)\n",
    "\n",
    "  # Create Corpus: Term Document Frequency\n",
    "  corpus = [id2word.doc2bow(text) for text in trigram_sent]\n",
    "  num_docs = id2word.num_docs\n",
    "  num_terms = len(id2word.keys())\n",
    "\n",
    "  # use 3 b/c len(labels)==3\n",
    "  token_labels = np.zeros(shape=(num_terms, 3))\n",
    "  for doc_idx in range(num_docs):\n",
    "    doc = corpus[doc_idx]\n",
    "    doc_label = data[doc_idx]['label']\n",
    "    for id, freq in doc:\n",
    "      token_labels[id][doc_label] += 1\n",
    "  columns = []\n",
    "  for i in id2word.keys():\n",
    "      columns.append(id2word[i])\n",
    "\n",
    "  # save stat for training set\n",
    "  token_stat = {\n",
    "      \"token_labels\": token_labels.tolist(),\n",
    "      \"token_list\": columns\n",
    "  }\n",
    "  with open('<specify your path>/mnli_government_travel/train_token_stat.json', 'w') as json_output:\n",
    "    json_output.write(json.dumps(token_stat))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "02-doc_process (train+test).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
